{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to horqrux","text":"<p>horqrux is a state vector and density matrix simulator designed for quantum machine learning written in JAX.</p>"},{"location":"#setup","title":"Setup","text":"<p>To install <code>horqrux</code> , you can go into any virtual environment of your choice and install it normally with <code>pip</code>:</p> <pre><code>pip install horqrux\n</code></pre>"},{"location":"#digital-operations","title":"Digital operations","text":"<p><code>horqrux</code> implements a large selection of both primitive and parametric single to n-qubit, digital quantum gates.</p> <p>Let's have a look at primitive gates first.</p> <pre><code>from horqrux import X, random_state, apply_gates\n\nstate = random_state(2)\nnew_state = apply_gates(state, X(0))\n</code></pre> <p>We can also make any gate controlled, in the case of X, we have to pass the target qubit first!</p> <pre><code>import jax.numpy as jnp\nfrom horqrux import X, product_state, apply_gates\n\nn_qubits = 2\nstate = product_state('11')\ncontrol = 0\ntarget = 1\n# This is equivalent to performing CNOT(0,1)\nnew_state= apply_gates(state, X(target,control))\nassert jnp.allclose(new_state, product_state('10'))\n</code></pre> <p>When applying parametric gates, we can either pass a numeric value or a parameter name for the parameter as the first argument.</p> <pre><code>import jax.numpy as jnp\nfrom horqrux import RX, random_state, apply_gates\n\ntarget_qubit = 1\nstate = random_state(target_qubit+1)\nparam_value = 1 / 4 * jnp.pi\nnew_state = apply_gates(state, RX(param_value, target_qubit))\n# Parametric horqrux gates also accept parameter names in the form of strings.\n# Simply pass a dictionary of parameter names and values to the 'apply_gates' function\nnew_state = apply_gates(state, RX('theta', target_qubit), {'theta': jnp.pi})\n</code></pre> <p>We can also make any parametric gate controlled simply by passing a control qubit.</p> <pre><code>import jax.numpy as jnp\nfrom horqrux import RX, product_state, apply_gates\n\nn_qubits = 2\ntarget_qubit = 1\ncontrol_qubit = 0\nstate = product_state('11')\nparam_value = 1 / 4 * jnp.pi\nnew_state = apply_gates(state, RX(param_value, target_qubit, control_qubit))\n</code></pre>"},{"location":"#using-sparse-matrices","title":"Using sparse matrices","text":"<p><code>horqrux</code> also provide the possibility to use sparse matrices when performing operations using Batched-coordinate (BCOO) sparse matrices. Note though that Jax's sparse matrices are still considered an experimental feature. For this, the input state and all operations should be initialized with <code>sparse=True</code>. One can also perform the sparse conversion of operators using the <code>to_sparse</code> method. And one can perform the reverse conversion using the <code>to_dense</code> method.</p> <pre><code>import jax.numpy as jnp\nfrom horqrux import RX, product_state, apply_gates\nfrom horqrux.utils.conversion import to_sparse, to_dense\n\nn_qubits = 2\ntarget_qubit = 1\ncontrol_qubit = 0\nstate = product_state('11', sparse=True)\nparam_value = 1 / 4 * jnp.pi\ngate = to_sparse(RX(param_value, target_qubit, control_qubit))\nnew_state = apply_gates(state, gate)\n\ngate = to_dense(RX(param_value, target_qubit, control_qubit))\nstate = to_dense(state)\nnew_state = apply_gates(state, gate)\n</code></pre> <p>Experimental Sparse matrices scope</p> <p>Note this is an experimental feature (raise an issue if needed). We only support noiseless state-vector simulation with digital operations when using sparse matrices. Note that <code>jax.grad</code> or <code>jax.experimental.sparse.grad</code> does not work with sparse expectation operations.</p>"},{"location":"#analog-operations","title":"Analog Operations","text":"<p><code>horqrux</code> also allows for global state evolution via the <code>HamiltonianEvolution</code> operation. Note that it expects a hamiltonian and a time evolution parameter passed as <code>numpy</code> or <code>jax.numpy</code> arrays. To build arbitrary Pauli hamiltonians, we recommend using Qadence.</p> <pre><code>from jax.numpy import pi, array, diag, kron, cdouble\nfrom horqrux.analog import HamiltonianEvolution\nfrom horqrux.apply import apply_gates\nfrom horqrux.utils.operator_utils import uniform_state\n\nsigmaz = diag(array([1.0, -1.0], dtype=cdouble))\nHbase = kron(sigmaz, sigmaz)\n\nHamiltonian = kron(Hbase, Hbase)\nn_qubits = 4\nt_evo = pi / 4\nhamevo = HamiltonianEvolution(tuple([i for i in range(n_qubits)]))\npsi = uniform_state(n_qubits)\npsi_star = apply_gates(psi, hamevo, {\"hamiltonian\": Hamiltonian, \"time_evolution\": t_evo})\n</code></pre>"},{"location":"#circuits","title":"Circuits","text":"<p>Using digital and analog operations, you can can build fully differentiable quantum circuits using the <code>QuantumCircuit</code> class.</p> <pre><code>import jax.numpy as jnp\nfrom horqrux import Z, RX, RY, NOT, expectation\nfrom horqrux.circuit import QuantumCircuit\nfrom horqrux.composite import Observable\nfrom horqrux.utils.operator_utils import zero_state\n\nops = [RX(\"theta\", 0), RY(\"epsilon\", 0), RX(\"phi\", 0), NOT(1, 0), RX(\"omega\", 0, 1)]\ncircuit = QuantumCircuit(2, ops)\nobservable = [Observable([Z(0)])]\nvalues = {\n    \"theta\": 0.2,\n    \"epsilon\": 0.3,\n    \"phi\": 0.4,\n    \"omega\": 0.5,\n}\nstate = zero_state(2)\nexp_qc = expectation(state, circuit, observable, values)\n</code></pre>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CONTRIBUTING/","title":"How to Contribute","text":"<p>We're grateful for your interest in participating in horqrux! Please follow our guidelines to ensure a smooth contribution process.</p>"},{"location":"CONTRIBUTING/#reporting-an-issue-or-proposing-a-feature","title":"Reporting an Issue or Proposing a Feature","text":"<p>Your course of action will depend on your objective, but generally, you should start by creating an issue. If you've discovered a bug or have a feature you'd like to see added to horqrux, feel free to create an issue on horqrux's GitHub issue tracker. Here are some steps to take:</p> <ol> <li>Quickly search the existing issues using relevant keywords to ensure your issue hasn't been addressed already.</li> <li> <p>If your issue is not listed, create a new one. Try to be as detailed and clear as possible in your description.</p> </li> <li> <p>If you're merely suggesting an improvement or reporting a bug, that's already excellent! We thank you for it. Your issue will be listed and, hopefully, addressed at some point.</p> </li> <li>However, if you're willing to be the one solving the issue, that would be even better! In such instances, you would proceed by preparing a Pull Request.</li> </ol>"},{"location":"CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<p>We're excited that you're eager to contribute to horqrux! To contribute, fork the <code>main</code> branch of horqrux repository and once you are satisfied with your feature and all the tests pass create a Pull Request.</p> <p>Here's the process for making a contribution:</p> <p>Click the \"Fork\" button at the upper right corner of the repo page to create a new GitHub repo at <code>https://github.com/USERNAME/horqrux</code>, where <code>USERNAME</code> is your GitHub ID. Then, <code>cd</code> into the directory where you want to place your new fork and clone it:</p> <pre><code>git clone https://github.com/USERNAME/horqrux.git\n</code></pre> <p>Next, navigate to your new horqrux fork directory and mark the main horqrux repository as the <code>upstream</code>:</p> <pre><code>git remote add upstream https://github.com/pasqal-io/horqrux.git\n</code></pre>"},{"location":"CONTRIBUTING/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>We recommended to use <code>hatch</code> for managing environments:</p> <p>To develop within horqrux, use: <pre><code>pip install hatch\nhatch -v shell\n</code></pre></p> <p>To run horqrux tests, use:</p> <pre><code>hatch -e tests run test\n</code></pre> <p>If you don't want to use <code>hatch</code>, you can use the environment manager of your choice (e.g. Conda) and execute the following:</p> <pre><code>pip install pytest\n\npip install -e .\npytest\n</code></pre>"},{"location":"CONTRIBUTING/#useful-things-for-your-workflow-linting-and-testing","title":"Useful Things for your workflow: Linting and Testing","text":"<p>Use <code>pre-commit</code> hooks to make sure that the code is properly linted before pushing a new commit. Make sure that the unit tests and type checks are passing since the merge request will not be accepted if the automatic CI/CD pipeline do not pass.</p> <p>Without <code>hatch</code>:</p> <pre><code>pip install pytest\n\npip install -e .\npip install pre-commit\npre-commit install\npre-commit run --all-files\npytest\n</code></pre> <p>And with <code>hatch</code>:</p> <pre><code>hatch -e tests run pre-commit run --all-files\nhatch -e tests run test\n</code></pre> <p>Make sure your docs build too!</p> <p>With <code>hatch</code>:</p> <pre><code>hatch -e docs run mkdocs build --clean --strict\n</code></pre> <p>Without <code>hatch</code>, <code>pip</code> install those libraries first: \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"mkdocs-section-index\", \"mkdocs-jupyter\", \"mkdocs-exclude\", \"markdown-exec\"</p> <p>And then:</p> <pre><code> mkdocs build --clean --strict\n</code></pre>"},{"location":"api/","title":"API","text":"<p><code>horqrux</code> exposes three API endpoints called <code>run</code>, <code>sample</code> and <code>expectation</code>.</p>"},{"location":"api/#run","title":"run","text":"Source code in <code>horqrux/api.py</code> <pre><code>def run(\n    circuit: OpSequence,\n    state: State,\n    values: dict[str, float] = dict(),\n) -&gt; State:\n    return circuit(state, values)\n</code></pre>"},{"location":"api/#sample","title":"sample","text":"<p>Sample from a quantum program.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>Input state vector or density matrix.</p> required <code>circuit</code> <code>OpSequence</code> <p>Sequence of gates.</p> required <code>values</code> <code>dict[str, float]</code> <p>description. Defaults to dict().</p> <code>dict()</code> <code>n_shots</code> <code>int</code> <p>Parameter values.. Defaults to 1000.</p> <code>1000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_shots &lt; 1.</p> <p>Returns:</p> Name Type Description <code>Counter</code> <code>Counter</code> <p>Bitstrings and frequencies.</p> Source code in <code>horqrux/api.py</code> <pre><code>def sample(\n    state: State,\n    circuit: OpSequence,\n    values: dict[str, float] = dict(),\n    n_shots: int = 1000,\n) -&gt; Counter:\n    \"\"\"Sample from a quantum program.\n\n    Args:\n        state (State): Input state vector or density matrix.\n        circuit (OpSequence): Sequence of gates.\n        values (dict[str, float], optional): _description_. Defaults to dict().\n        n_shots (int, optional): Parameter values.. Defaults to 1000.\n\n    Raises:\n        ValueError: If n_shots &lt; 1.\n\n    Returns:\n        Counter: Bitstrings and frequencies.\n    \"\"\"\n    if n_shots &lt; 1:\n        raise ValueError(\"You can only sample with non-negative 'n_shots'.\")\n    output_circuit = circuit(state, values)\n    n_qubits = num_qubits(output_circuit)\n    if isinstance(output_circuit, DensityMatrix):\n        d = 2**n_qubits\n        output_circuit.array = output_circuit.array.reshape((d, d))\n\n    probs = probabilities(output_circuit)\n    return sample_from_probs(probs, n_qubits, n_shots)\n</code></pre>"},{"location":"api/#expectation","title":"expectation","text":"<p>Run 'state' through a sequence of 'gates' given parameters 'values' and compute the expectation given an observable.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>Input state vector or density matrix.</p> required <code>circuit</code> <code>OpSequence</code> <p>Sequence of gates.</p> required <code>observables</code> <code>list[Observable]</code> <p>List of observables.</p> required <code>values</code> <code>dict[str, float] | dict[str, dict[str, float]]</code> <p>A dictionary containing &lt;'parameter_name': value&gt; pairs denoting the current parameter values for each parameter in <code>circuit</code>. Note it can include also values for the observables, but differentiation will not separate gradients. To do so, we should provide values as a dict with two keys: <code>circuit</code> and <code>observables</code>, each a dict.</p> <code>dict()</code> <code>diff_mode</code> <code>DiffMode</code> <p>Differentiation mode. Defaults to DiffMode.AD.</p> <code>AD</code> <code>n_shots</code> <code>int</code> <p>Number of shots. Defaults to 0 for no shots.</p> <code>0</code> <code>key</code> <code>Any</code> <p>Random key. Defaults to jax.random.PRNGKey(0).</p> <code>PRNGKey(0)</code> <p>Returns:</p> Name Type Description <code>Array</code> <code>Array</code> <p>Expectation values.</p> Source code in <code>horqrux/api.py</code> <pre><code>def expectation(\n    state: State,\n    circuit: OpSequence,\n    observables: list[Observable],\n    values: dict | dict[str, float] | dict[str, dict[str, float]] = dict(),\n    diff_mode: DiffMode = DiffMode.AD,\n    n_shots: int = 0,\n    key: Any = jax.random.PRNGKey(0),\n) -&gt; Array:\n    \"\"\"Run 'state' through a sequence of 'gates' given parameters 'values'\n    and compute the expectation given an observable.\n\n    Args:\n        state (State): Input state vector or density matrix.\n        circuit (OpSequence): Sequence of gates.\n        observables (list[Observable]): List of observables.\n        values (dict[str, float] | dict[str, dict[str, float]], optional): A dictionary\n            containing &lt;'parameter_name': value&gt; pairs\n            denoting the current parameter values for each parameter in `circuit`.\n            Note it can include also values for the observables, but differentiation will\n            not separate gradients.\n            To do so, we should provide values as a dict with two keys: `circuit` and `observables`, each a dict.\n        diff_mode (DiffMode, optional): Differentiation mode. Defaults to DiffMode.AD.\n        n_shots (int): Number of shots. Defaults to 0 for no shots.\n        key (Any, optional): Random key. Defaults to jax.random.PRNGKey(0).\n\n    Returns:\n        Array: Expectation values.\n    \"\"\"\n\n    if diff_mode == DiffMode.AD:\n        if n_shots &lt; 0:\n            raise ValueError(\"The number of shots should be positive.\")\n        if n_shots == 0:\n            return ad_expectation(state, circuit, observables, values)\n        else:\n            return finite_shots(\n                state=state,\n                gates=list(iter(circuit)),  # type: ignore[type-var]\n                observables=observables,\n                values=values,  # type: ignore[arg-type]\n                n_shots=n_shots,\n                key=key,\n            )\n    elif diff_mode == DiffMode.ADJOINT:\n        if isinstance(state, DensityMatrix):\n            raise TypeError(\"Adjoint does not support density matrices.\")\n\n        return adjoint_expectation(state, circuit, observables, values)\n\n    elif diff_mode == DiffMode.GPSR:\n        if n_shots &lt; 0:\n            raise ValueError(\"The number of shots should be positive.\")\n        if n_shots == 0:\n            return analytical_gpsr_fwd(\n                state=state,\n                gates=list(iter(circuit)),  # type: ignore[type-var]\n                observables=observables,\n                values=values,\n            )\n        else:\n            return finite_shots_fwd(\n                state=state,\n                gates=list(iter(circuit)),  # type: ignore[type-var]\n                observables=observables,\n                values=values,\n                n_shots=n_shots,\n                key=key,\n            )\n    else:\n        raise ValueError(f\"Differentiation mode {diff_mode} is not supported.\")\n</code></pre>"},{"location":"differentiation/","title":"Differentiation","text":""},{"location":"differentiation/#differentiation","title":"Differentiation","text":"<p><code>horqrux</code> also offers several differentiation modes to compute gradients which can be accessed through the <code>expectation</code> API. It requires to pass one of the three <code>DiffMode</code> options to the <code>diff_mode</code> argument. The default is <code>ad</code>.</p>"},{"location":"differentiation/#automatic-differentiation-diffmodead","title":"Automatic Differentiation (DiffMode.AD)","text":"<p>The default differentation mode of <code>horqrux</code> uses jax.grad, the <code>jax</code> native automatic differentiation engine which tracks operations on <code>jax.Array</code> objects by constructing a computational graph to perform chain rules for derivative calculations.</p> <p>Using shots</p> <p>When using shots, automatic differentiation does not apply as sampling break the computational graph.</p>"},{"location":"differentiation/#adjoint-differentiation-diffmodeadjoint","title":"Adjoint Differentiation (DiffMode.ADJOINT)","text":"<p>The adjoint differentiation mode computes first-order gradients by only requiring at most three states in memory in <code>O(P)</code> time where <code>P</code> is the number of parameters in a circuit.</p> <p>Usage restrictions</p> <p>Adjoint differentiation does not support shots or density matrix inputs.</p>"},{"location":"differentiation/#generalized-parameter-shift-rules-diffmodegpsr","title":"Generalized Parameter-Shift rules (DiffMode.GPSR)","text":"<p>The Generalized parameter shift rule (GPSR mode) is an extension of the well known parameter shift rule (PSR) algorithm to arbitrary quantum operations. Indeed, PSR applies for quantum operations whose generator has a single gap in its eigenvalue spectrum. GPSR extends to multi-gap eigenvalued generators.</p> <p>Usage restrictions</p> <p>At the moment, circuits with one or more <code>Scale</code> and/or <code>HamiltonianEvolution</code> operations are not supported. They should be handled differently as GPSR requires operations to be of the form presented below.</p> <p>For this, we define the differentiable function as quantum expectation value:</p> \\[ f(x) = \\left\\langle 0\\right|\\hat{U}^{\\dagger}(x)\\hat{C}\\hat{U}(x)\\left|0\\right\\rangle \\] <p>where \\(\\hat{U}(x)={\\rm exp}{\\left( -i\\frac{x}{2}\\hat{G}\\right)}\\) is the quantum evolution operator with generator \\(\\hat{G}\\) representing the structure of the underlying quantum circuit and \\(\\hat{C}\\) is the cost operator. Then using the eigenvalue spectrum \\(\\left\\{ \\lambda_n\\right\\}\\) of the generator \\(\\hat{G}\\) we calculate the full set of corresponding unique non-zero spectral gaps \\(\\left\\{ \\Delta_s\\right\\}\\) (differences between eigenvalues). It can be shown that the final expression of derivative of \\(f(x)\\) is then given by the following expression:</p> <p>\\(\\begin{equation} \\frac{{\\rm d}f\\left(x\\right)}{{\\rm d}x}=\\overset{S}{\\underset{s=1}{\\sum}}\\Delta_{s}R_{s}, \\end{equation}\\)</p> <p>where \\(S\\) is the number of unique non-zero spectral gaps and \\(R_s\\) are real quantities that are solutions of a system of linear equations</p> <p>\\(\\begin{equation} \\begin{cases} F_{1} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{1}\\Delta_{s}}{2}\\right)R_{s},\\\\ F_{2} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{2}\\Delta_{s}}{2}\\right)R_{s},\\\\  &amp; ...\\\\ F_{S} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{M}\\Delta_{s}}{2}\\right)R_{s}. \\end{cases} \\end{equation}\\)</p> <p>Here \\(F_s=f(x+\\delta_s)-f(x-\\delta_s)\\) denotes the difference between values of functions evaluated at shifted arguments \\(x\\pm\\delta_s\\).</p>"},{"location":"differentiation/#examples","title":"Examples","text":""},{"location":"differentiation/#circuit-parameters-differentiation","title":"Circuit parameters differentiation","text":"<p>We show below a code example with several differentiation methods for circuit parameters. Note that jax.grad requires functions of <code>Array</code>.</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom jax import Array\n\nfrom horqrux import expectation, random_state, DiffMode\nfrom horqrux.circuit import QuantumCircuit\nfrom horqrux.composite import Observable\nfrom horqrux.primitives.parametric import RX\nfrom horqrux.primitives.primitive import Z\n\nN_QUBITS = 2\n\nx = jax.random.uniform(jax.random.key(0), (2,))\n\nparam_prefix = \"theta\"\nparam_names = [param_prefix, param_prefix + \"2\"]\nops = [RX(param_names[0], 0), RX(param_names[1], 1)]\n\ndef values_to_dict(x: Array) -&gt; dict[str, Array]:\n    return {param_names[0]: x[0], param_names[1]: x[1]}\n\ncircuit = QuantumCircuit(2, ops)\nobservables = [Observable([Z(0)]), Observable([Z(1)])]\nstate = random_state(N_QUBITS)\n\ndef expectation_ad(x: Array) -&gt; Array:\n    values = values_to_dict(x)\n    return expectation(state, circuit, observables, values, diff_mode=DiffMode.AD).sum()\n\ndef expectation_gpsr(x: Array) -&gt; Array:\n    values = values_to_dict(x)\n    return expectation(state, circuit, observables, values, diff_mode=DiffMode.GPSR).sum()\n\ndef expectation_adjoint(x: Array) -&gt; Array:\n    values = values_to_dict(x)\n    return expectation(state, circuit, observables, values, diff_mode= DiffMode.ADJOINT).sum()\n\nd_ad = jax.grad(expectation_ad)\nd_gpsr = jax.grad(expectation_gpsr)\nd_adjoint = jax.grad(expectation_adjoint)\n\ngrad_ad = d_ad(x)\ngrad_gpsr = d_gpsr(x)\ngrad_adjoint = d_adjoint(x)\n</code></pre>   Gradient: [ 0.66915283 -0.81851535]"},{"location":"differentiation/#parametrized-observable-differentiation","title":"Parametrized observable differentiation","text":"<p>To allow differentiating observable parameters only, we need to specify the <code>values</code> argument as a dictionary with two keys <code>circuit</code> and <code>observables</code>, each being a dictionary of corresponding parameters and values, as follows:</p> <pre><code>from horqrux.primitives.parametric import RZ\nobservables = [Observable([RZ(param_prefix + \"_obs\", 0)])]\nobsval = jax.random.uniform(jax.random.key(0), (1,))\n\n\ndef expectation_separate_parameters(x: Array, y: Array) -&gt; Array:\n    values = {\"circuit\": values_to_dict(x), \"observables\": {param_prefix + \"_obs\": y}}\n    return expectation(state, circuit, observables, values, diff_mode=DiffMode.AD).sum()\n\ndobs_ad = jax.grad(expectation_separate_parameters, argnums=1)\ngrad_ad = dobs_ad(x, obsval)\n</code></pre>   Gradient: [-0.10385267]"},{"location":"dqc/","title":"Fitting a nonlinear function using adjoint differentiation","text":"<p>We can build a fully differentiable variational circuit by defining a sequence of gates and a set of optimizable parameter values. <code>horqrux</code> provides an implementation for the adjoint differentiation method<sup>1</sup>, which we can use to fit a function using a simple <code>FunctionFitter</code> class inheriting from the <code>QuantumCircuit</code> class.</p> <pre><code>from __future__ import annotations\n\nimport jax\nfrom jax import grad, jit, Array, value_and_grad, vmap\nfrom jax.tree_util import register_pytree_node_class\n\nfrom dataclasses import dataclass\nimport jax.numpy as jnp\nimport optax\nfrom functools import reduce, partial\nfrom operator import add\nfrom typing import Any, Callable\nfrom uuid import uuid4\n\nfrom horqrux import expectation, Observable\nfrom horqrux import Z, RX, RY, NOT, zero_state, apply_gates\nfrom horqrux.circuit import QuantumCircuit, hea\nfrom horqrux.primitives.primitive import Primitive\nfrom horqrux.primitives.parametric import Parametric\nfrom horqrux.utils.operator_utils import DiffMode\n\n\nn_qubits = 5\nn_params = 3\nn_layers = 3\n\n#  We need a target function to fit and to produce training data\nfn = lambda x, degree: .05 * reduce(add, (jnp.cos(i*x) + jnp.sin(i*x) for i in range(degree)), 0)\nx = jnp.linspace(0, 10, 100)\ny = fn(x, 5)\n\nclass FunctionFitter(QuantumCircuit):\n    \"\"\"\n    The FunctionFitter is composed of a quantum circuit and an observable to obtain a real-valued output.\n    It can be seen as a function of input values `x` that are passed as parameter values of a subset of parameterized quantum gates.\n    The output is define as\n    The rest of the parameterized quantum gates use the `values` coming from a classical optimizer.\n\n    Attributes:\n        n_qubits (int): Number of qubits.\n        operations (list[Primitive]): Operations defining the circuit.\n        fparams (list[str]): List of parameters that are considered\n            non trainable, used for passing fixed input data to a quantum circuit.\n            The corresponding operations compose the `feature map`.\n        observable (Observable): Observable for getting real-valued measurement output.\n            Here, we use the Z observable applied on qubit 0.\n        state (Array): Initial zero state.\n    \"\"\"\n    def __init__(self, n_qubits, operations, fparams) -&gt; None:\n        super().__init__(n_qubits, operations, fparams)\n        self.observable: Observable = Observable([Z(0)])\n        self.state = zero_state(self.n_qubits)\n\n    @partial(vmap, in_axes=(None, None, 0))\n    def __call__(self, param_values: Array, x: Array) -&gt; Array:\n        param_dict = {name: val for name, val in zip(self.vparams, param_values)}\n        return jnp.squeeze(expectation(self.state, self, [self.observable], {**param_dict, **{'phi': x}}, DiffMode.ADJOINT))\n\nfeature_map = [RX('phi', i) for i in range(n_qubits)]\nfm_names = [f.param for f in feature_map]\nansatz = hea(n_qubits, n_layers)\ncirc = FunctionFitter(n_qubits, feature_map + ansatz, fm_names)\n# Create random initial values for the parameters\nkey = jax.random.PRNGKey(42)\nparam_vals = jax.random.uniform(key, shape=(circ.n_vparams,))\n# Check the initial predictions using randomly initialized parameters\ny_init = circ(param_vals, x)\n\noptimizer = optax.adam(learning_rate=0.01)\nopt_state = optimizer.init(param_vals)\n\n# Define a loss function\ndef loss_fn(param_vals: Array) -&gt; Array:\n    y_pred = circ(param_vals, x)\n    return jnp.mean(optax.l2_loss(y_pred, y))\n\n\ndef optimize_step(param_vals: Array, opt_state: Array, grads: Array) -&gt; tuple:\n    updates, opt_state = optimizer.update(grads, opt_state)\n    param_vals = optax.apply_updates(param_vals, updates)\n    return param_vals, opt_state\n\n@jit\ndef train_step(i: int, paramvals_w_optstate: tuple) -&gt; tuple:\n    param_vals, opt_state = paramvals_w_optstate\n    loss, grads = value_and_grad(loss_fn)(param_vals)\n    param_vals, opt_state = optimize_step(param_vals, opt_state, grads)\n    return param_vals, opt_state\n\n\nn_epochs = 200\nparam_vals, opt_state = jax.lax.fori_loop(0, n_epochs, train_step, (param_vals, opt_state))\ny_final = circ(param_vals, x)\n\n# Lets plot the results\nimport matplotlib.pyplot as plt\nplt.plot(x, y, label=\"truth\")\nplt.plot(x, y_init, label=\"initial\")\nplt.plot(x, y_final, \"--\", label=\"final\", linewidth=3)\nplt.legend()\n</code></pre> 2025-05-26T09:38:00.147834 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/"},{"location":"dqc/#fitting-a-partial-differential-equation-using-differentiablequantumcircuit","title":"Fitting a partial differential equation using DifferentiableQuantumCircuit","text":"<p>We show how a Differentiable Quantum Circuit (DQC)<sup>2</sup> can be implemented in <code>horqrux</code> and solve a partial differential equation. To do so, we define a <code>PDESolver</code> class inheriting from the <code>QuantumCircuit</code> class.</p> <pre><code>from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom functools import reduce\nfrom itertools import product\nfrom operator import add\nfrom typing import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\nfrom jax import Array, jit, value_and_grad, vmap\nfrom numpy.random import uniform\n\nfrom horqrux.apply import group_by_index\nfrom horqrux.circuit import QuantumCircuit, hea\nfrom horqrux import NOT, RX, RY, Z, apply_gates, zero_state, Observable\nfrom horqrux.primitives.primitive import Primitive\nfrom horqrux.primitives.parametric import Parametric\nfrom horqrux.utils.operator_utils import inner\n\nLEARNING_RATE = 0.01\nN_QUBITS = 4\nDEPTH = 3\nVARIABLES = (\"x\", \"y\")\nNUM_VARIABLES = len(VARIABLES)\nX_POS, Y_POS = [i for i in range(NUM_VARIABLES)]\nBATCH_SIZE = 500\nN_EPOCHS = 500\n\ndef total_magnetization(n_qubits:int) -&gt; Callable:\n    paulis = Observable([Z(i) for i in range(n_qubits)])\n\n    def _total_magnetization(out_state: Array, values: dict[str, Array]) -&gt; Array:\n        projected_state = paulis.forward(out_state, values)\n        return inner(out_state, projected_state).real\n    return _total_magnetization\n\nclass PDESolver(QuantumCircuit):\n    \"\"\"\n    The PDESolver is composed of a quantum circuit and an observable to obtain a real-valued output.\n    It can be seen as a function of input values `x`, `y` that are passed as parameter values of a subset of parameterized quantum gates.\n    The rest of the parameterized quantum gates use the `values` coming from a classical optimizer.\n\n    Attributes:\n        n_qubits (int): Number of qubits.\n        operations (list[Primitive]): Operations defining the circuit.\n        fparams (list[str]): List of parameters that are considered\n            non trainable, used for passing fixed input data to a quantum circuit.\n                The corresponding operations compose the `feature map`.\n        observable (Callable): Function applying the observable for getting real-valued output.\n            Here, we use the total_magnetization function applying the observable \\sum_i^N Z(i).\n        state (Array): Initial zero state.\n    \"\"\"\n    def __init__(self, n_qubits, operations, fparams) -&gt; None:\n        operations = group_by_index(operations)\n        super().__init__(n_qubits, operations, fparams)\n        self.observable = total_magnetization(self.n_qubits)\n        self.state = zero_state(self.n_qubits)\n\n    def __call__(self, values: dict[str, Array], x: Array, y: Array) -&gt; Array:\n        param_dict = {name: val for name, val in zip(self.vparams, values)}\n        out_state = apply_gates(\n            self.state, self.operations, {**param_dict, **{\"f_x\": x, \"f_y\": y}}\n        )\n        return self.observable(out_state, {})\n\n\nfm =  [RX(\"f_x\", i) for i in range(N_QUBITS // 2)] + [\n            RX(\"f_y\", i) for i in range(N_QUBITS // 2, N_QUBITS)\n        ]\nfm_circuit_parameters = [f.param for f in fm]\nansatz = hea(N_QUBITS, DEPTH)\ncirc = PDESolver(N_QUBITS, fm + ansatz, fm_circuit_parameters)\n# Create random initial values for the parameters\nkey = jax.random.PRNGKey(42)\nparam_vals = jax.random.uniform(key, shape=(circ.n_vparams,))\n\noptimizer = optax.adam(learning_rate=LEARNING_RATE)\nopt_state = optimizer.init(param_vals)\n\n\ndef loss_fn(param_vals: Array) -&gt; Array:\n    def pde_loss(x: Array, y: Array) -&gt; Array:\n        x = x.reshape(-1, 1)\n        y = y.reshape(-1, 1)\n        left = (jnp.zeros_like(y), y)  # u(0,y)=0\n        right = (jnp.ones_like(y), y)  # u(L,y)=0\n        top = (x, jnp.ones_like(x))  # u(x,H)=0\n        bottom = (x, jnp.zeros_like(x))  # u(x,0)=f(x)\n        terms = jnp.dstack(list(map(jnp.hstack, [left, right, top, bottom])))\n        loss_left, loss_right, loss_top, loss_bottom = vmap(lambda xy: circ(param_vals, xy[:, 0], xy[:, 1]), in_axes=(2,))(\n            terms\n        )\n        loss_bottom -= jnp.sin(jnp.pi * x)\n        hessian = jax.hessian(lambda xy: circ(param_vals, xy[0], xy[1]))(\n            jnp.concatenate(\n                [\n                    x.reshape(\n                        1,\n                    ),\n                    y.reshape(\n                        1,\n                    ),\n                ]\n            )\n        )\n        loss_interior = hessian[X_POS][X_POS] + hessian[Y_POS][Y_POS]  # uxx+uyy=0\n        return jnp.sum(\n            jnp.concatenate(\n                list(\n                    map(\n                        lambda term: jnp.power(term, 2).reshape(-1, 1),\n                        [loss_left, loss_right, loss_top, loss_bottom, loss_interior],\n                    )\n                )\n            )\n        )\n\n    return jnp.mean(vmap(pde_loss, in_axes=(0, 0))(*uniform(0, 1.0, (NUM_VARIABLES, BATCH_SIZE))))\n\n\ndef optimize_step(param_vals: Array, opt_state: Array, grads: dict[str, Array]) -&gt; tuple:\n    updates, opt_state = optimizer.update(grads, opt_state, param_vals)\n    param_vals = optax.apply_updates(param_vals, updates)\n    return param_vals, opt_state\n\n\n@jit\ndef train_step(i: int, paramvals_w_optstate: tuple) -&gt; tuple:\n    param_vals, opt_state = paramvals_w_optstate\n    loss, grads = value_and_grad(loss_fn)(param_vals)\n    return optimize_step(param_vals, opt_state, grads)\n\n\nparam_vals, opt_state = jax.lax.fori_loop(0, N_EPOCHS, train_step, (param_vals, opt_state))\n# compare the solution to known ground truth\nsingle_domain = jnp.linspace(0, 1, num=BATCH_SIZE)\ndomain = jnp.array(list(product(single_domain, single_domain)))\n# analytical solution\nanalytic_sol = (\n    (np.exp(-np.pi * domain[:, 0]) * np.sin(np.pi * domain[:, 1])).reshape(BATCH_SIZE, BATCH_SIZE).T\n)\n# DQC solution\ndqc_sol = vmap(lambda domain: circ(param_vals, domain[0], domain[1]), in_axes=(0,))(\n    domain\n).reshape(BATCH_SIZE, BATCH_SIZE)\n# # plot results\nfig, ax = plt.subplots(1, 2, figsize=(7, 7))\nax[0].imshow(analytic_sol, cmap=\"turbo\")\nax[0].set_xlabel(\"x\")\nax[0].set_ylabel(\"y\")\nax[0].set_title(\"Analytical solution u(x,y)\")\nax[1].imshow(dqc_sol, cmap=\"turbo\")\nax[1].set_xlabel(\"x\")\nax[1].set_ylabel(\"y\")\nax[1].set_title(\"DQC solution u(x,y)\")\n</code></pre> 2025-05-26T09:39:43.750532 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/ <ol> <li> <p>Tyson Jones, Julien Gacon , Efficient calculation of gradients in classical simulations of variational quantum algorithms  (2020) \u21a9</p> </li> <li> <p>Oleksandr Kyriienko, Annie E. Paine, Vincent E. Elfving, Solving nonlinear differential equations with differentiable quantum circuits  (2020) \u21a9</p> </li> </ol>"},{"location":"noise/","title":"Noisy simulation","text":""},{"location":"noise/#digital-noise","title":"Digital Noise","text":"<p>In the description of closed quantum systems, the complete quantum state is a pure quantum state represented by a state vector $|\\psi \\rangle $.</p> <p>However, this description is not sufficient to describe open quantum systems. When the system interacts with its surrounding environment, it transitions into a mixed state where quantum information is no longer entirely contained in a single state vector but is distributed probabilistically.</p> <p>To address these more general cases, we consider a probabilistic combination \\(p_i\\) of possible pure states \\(|\\psi_i \\rangle\\). Thus, the system is described by a density matrix \\(\\rho\\) defined as follows:</p> \\[ \\rho = \\sum_i p_i |\\psi_i\\rangle \\langle \\psi_i| \\] <p>The transformations of the density operator of an open quantum system interacting with its noisy environment are represented by the super-operator \\(S: \\rho \\rightarrow S(\\rho)\\), often referred to as a quantum channel. Quantum channels, due to the conservation of the probability distribution, must be CPTP (Completely Positive and Trace Preserving). Any CPTP super-operator can be written in the following form:</p> \\[ S(\\rho) = \\sum_i K_i \\rho K^{\\dagger}_i \\] <p>Where \\(K_i\\) are Kraus operators satisfying the closure property \\(\\sum_i K_i K^{\\dagger}_i = \\mathbb{I}\\). As noise is the result of system interactions with its environment, it is therefore possible to simulate noisy quantum circuit with noise type gates.</p> <p>Thus, <code>horqrux</code> implements a large selection of single qubit noise gates such as:</p> <ul> <li>The bit flip channel defined as: \\(\\textbf{BitFlip}(\\rho) =(1-p) \\rho + p X \\rho X^{\\dagger}\\)</li> <li>The phase flip channel defined as: \\(\\textbf{PhaseFlip}(\\rho) = (1-p) \\rho + p Z \\rho Z^{\\dagger}\\)</li> <li>The depolarizing channel defined as: \\(\\textbf{Depolarizing}(\\rho) = (1-p) \\rho + \\frac{p}{3} (X \\rho X^{\\dagger} + Y \\rho Y^{\\dagger} + Z \\rho Z^{\\dagger})\\)</li> <li>The pauli channel defined as: \\(\\textbf{PauliChannel}(\\rho) = (1-p_x-p_y-p_z) \\rho             + p_x X \\rho X^{\\dagger}             + p_y Y \\rho Y^{\\dagger}             + p_z Z \\rho Z^{\\dagger}\\)</li> <li>The amplitude damping channel defined as: \\(\\textbf{AmplitudeDamping}(\\rho) =  K_0 \\rho K_0^{\\dagger} + K_1 \\rho K_1^{\\dagger}\\)     with:     \\(\\begin{equation*}     K_{0} \\ =\\begin{pmatrix}     1 &amp; 0\\\\     0 &amp; \\sqrt{1-\\ \\gamma }     \\end{pmatrix} ,\\ K_{1} \\ =\\begin{pmatrix}     0 &amp; \\sqrt{\\ \\gamma }\\\\     0 &amp; 0     \\end{pmatrix}     \\end{equation*}\\)</li> <li>The phase damping channel defined as: \\(\\textbf{PhaseDamping}(\\rho) = K_0 \\rho K_0^{\\dagger} + K_1 \\rho K_1^{\\dagger}\\)     with:     \\(\\begin{equation*}     K_{0} \\ =\\begin{pmatrix}     1 &amp; 0\\\\     0 &amp; \\sqrt{1-\\ \\gamma }     \\end{pmatrix}, \\ K_{1} \\ =\\begin{pmatrix}     0 &amp; 0\\\\     0 &amp; \\sqrt{\\ \\gamma }     \\end{pmatrix}     \\end{equation*}\\)</li> <li>The generalize amplitude damping channel is defined as: \\(\\textbf{GeneralizedAmplitudeDamping}(\\rho) = K_0 \\rho K_0^{\\dagger} + K_1 \\rho K_1^{\\dagger} + K_2 \\rho K_2^{\\dagger} + K_3 \\rho K_3^{\\dagger}\\)     with: \\(\\begin{cases} K_{0} \\ =\\sqrt{p} \\ \\begin{pmatrix} 1 &amp; 0\\\\ 0 &amp; \\sqrt{1-\\ \\gamma } \\end{pmatrix} ,\\ K_{1} \\ =\\sqrt{p} \\ \\begin{pmatrix} 0 &amp; 0\\\\ 0 &amp; \\sqrt{\\ \\gamma } \\end{pmatrix} \\\\ K_{2} \\ =\\sqrt{1\\ -p} \\ \\begin{pmatrix} \\sqrt{1-\\ \\gamma } &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix} ,\\ K_{3} \\ =\\sqrt{1-p} \\ \\begin{pmatrix} 0 &amp; 0\\\\ \\sqrt{\\ \\gamma } &amp; 0 \\end{pmatrix} \\end{cases}\\)</li> </ul> <p>Noise protocols can be added to gates by instantiating <code>DigitalNoiseInstance</code> providing the <code>DigitalNoiseType</code> and the <code>error_probability</code> (either float or tuple of float):</p> <pre><code>from horqrux.noise import DigitalNoiseInstance, DigitalNoiseType\n\nnoise_prob = 0.3\nAmpD = DigitalNoiseInstance(DigitalNoiseType.AMPLITUDE_DAMPING, error_probability=noise_prob)\n</code></pre> <p>Then a gate can be instantiated by providing a tuple of <code>DigitalNoiseInstance</code> instances. Let\u2019s show this through the simulation of a realistic \\(X\\) gate.</p> <p>For instance, an \\(X\\) gate flips the state of the qubit: \\(X|0\\rangle = |1\\rangle\\). In practice, it's common for the target qubit to stay in its original state after applying \\(X\\) due to the interactions between it and its environment. The possibility of failure can be represented by the <code>BitFlip</code> subclass of <code>DigitalNoiseInstance</code>, which flips the state again after the application of the \\(X\\) gate, returning it to its original state with a probability <code>1 - gate_fidelity</code>.</p> <pre><code>from horqrux import X, QuantumCircuit\nfrom horqrux.api import sample\nfrom horqrux.noise import DigitalNoiseInstance, DigitalNoiseType\nfrom horqrux.utils.operator_utils import density_mat, product_state\n\nnoise = (DigitalNoiseInstance(DigitalNoiseType.BITFLIP, 0.1),)\nops = QuantumCircuit(1, [X(0)])\nnoisy_ops = QuantumCircuit(1, [X(0, noise=noise)])\nstate = product_state(\"0\")\n\nnoiseless_samples = sample(state, ops)\nnoisy_samples = sample(density_mat(state), noisy_ops)\n</code></pre>   Noiseless samples: Counter({'1': 1000}) Noiseless samples: Counter({'1': 900, '0': 100})"},{"location":"vqe/","title":"Variational quantum eigensolver","text":"<p>We will demonstrate how to perform a Variational quantum eigensolver (VQE)<sup>1</sup> task on a molecular example using Horqrux. VQE boils down to finding the molecular ground state \\(| \\psi(\\theta) \\rangle\\) that minimizes the energy with respect to a molecular hamiltonian of interest denoted \\(H\\) : \\(\\(\\langle \\psi(\\theta) | H | \\psi(\\theta) \\rangle\\)\\)</p>"},{"location":"vqe/#hamiltonian","title":"Hamiltonian","text":"<p>In this example, we run VQE for the \\(H2\\) molecule in the STO-3G basis with a bondlength of \\(0.742 \\mathring{A}\\)<sup>2</sup>. The groud-state energy is around \\(-1.137\\). Note we need to manually create it by hand, as no syntax method is implemented (such methods are available in <code>Qadence</code> though).</p> <pre><code>import jax\nfrom jax import Array\nimport optax\n\nimport time\n\nimport horqrux\nfrom horqrux import I, Z, X, Y\nfrom horqrux import Scale, Observable\nfrom horqrux.composite import OpSequence\n\nfrom horqrux.api import expectation\nfrom horqrux.circuit import hea, QuantumCircuit\n\nH2_hamiltonian = Observable([\n  Scale(I(0), -0.09963387941370971),\n  Scale(Z(0), 0.17110545123720233),\n  Scale(Z(1), 0.17110545123720225),\n  Scale(OpSequence([Z(0) , Z(1)]), 0.16859349595532533),\n  Scale(OpSequence([Y(0) , X(1) , X(2) , Y(3)]), 0.04533062254573469),\n  Scale( OpSequence([Y(0) , Y(1) , X(2) , X(3)]) , -0.04533062254573469),\n  Scale( OpSequence([X(0) , X(1) , Y(2) , Y(3)]) , -0.04533062254573469),\n  Scale( OpSequence([X(0) , Y(1) , Y(2) , X(3)]),  0.04533062254573469),\n  Scale(Z(2),-0.22250914236600539),\n  Scale( OpSequence([Z(0) , Z(2)]), 0.12051027989546245),\n  Scale(Z(3), -0.22250914236600539),\n  Scale(OpSequence([Z(0) , Z(3)]), 0.16584090244119712),\n  Scale(OpSequence([Z(1) , Z(2)]), 0.16584090244119712),\n  Scale(OpSequence([Z(1) , Z(3)]), 0.12051027989546245),\n  Scale(OpSequence([Z(2) , Z(3)]), 0.1743207725924201),\n])\n</code></pre>"},{"location":"vqe/#ansatz","title":"Ansatz","text":"<p>As an ansatz, we use the hardware-efficient ansatz<sup>3</sup> with \\(5\\) layers applied on the initial state \\(| 0011 \\rangle\\).</p> <pre><code>init_state = horqrux.product_state(\"0011\")\nansatz = QuantumCircuit(4, hea(4, 5))\n</code></pre>   Number of variational parameters:  60"},{"location":"vqe/#optimization-with-automatic-differentiation","title":"Optimization with automatic differentiation","text":"<p>The objective here is to optimize the variational parameters of our ansatz using the standard Adam optimizer. Below we show how to set up a train function. We first consider the non-jitted version of the training function to compare later the timing with the jitted-version.</p> <pre><code># Create random initial values for the parameters\nkey = jax.random.PRNGKey(42)\ninit_param_vals = jax.random.uniform(key, shape=(ansatz.n_vparams,))\nLEARNING_RATE = 0.01\nN_EPOCHS = 50\n\noptimizer = optax.adam(learning_rate=LEARNING_RATE)\n\ndef optimize_step(param_vals: Array, opt_state: Array, grads: dict[str, Array]) -&gt; tuple:\n    updates, opt_state = optimizer.update(grads, opt_state, param_vals)\n    param_vals = optax.apply_updates(param_vals, updates)\n    return param_vals, opt_state\n\ndef loss_fn(param_vals: Array) -&gt; Array:\n    \"\"\"The loss function is the sum of all expectation value for the observable components.\"\"\"\n    values = dict(zip(ansatz.vparams, param_vals))\n    return jax.numpy.sum(expectation(init_state, ansatz, observables=[H2_hamiltonian], values=values))\n\n\ndef train_step(i: int, param_vals_opt_state: tuple) -&gt; tuple:\n    param_vals, opt_state = param_vals_opt_state\n    loss, grads = jax.value_and_grad(loss_fn)(param_vals)\n    return optimize_step(param_vals, opt_state, grads)\n\n# set initial parameters and the state of the optimizer\nparam_vals = init_param_vals.clone()\nopt_state = optimizer.init(init_param_vals)\n\ndef train_unjitted(param_vals, opt_state):\n    for i in range(0, N_EPOCHS):\n        param_vals, opt_state = train_step(i, (param_vals, opt_state))\n    return param_vals, opt_state\n\nstart = time.time()\nparam_vals, opt_state = train_unjitted(param_vals, opt_state)\nend = time.time()\ntime_nonjit = end - start\n</code></pre>   Initial loss: -0.247 Final loss: -1.051    <p>Now, we will jit the <code>train_step</code> function with <code>jax.lax.fori_loop</code> and improve execution time (expecting at least \\(10\\) times faster, depending on system):</p> <pre><code># reset state and parameters\nparam_vals = init_param_vals.clone()\nopt_state = optimizer.init(param_vals)\n\nstart_jit = time.time()\nparam_vals, opt_state = jax.lax.fori_loop(0, N_EPOCHS, train_step, (param_vals, opt_state))\nend_jit = time.time()\ntime_jit = end_jit - start_jit\n\nprint(f\"Time speedup: {time_nonjit / time_jit:.3f}\")\n</code></pre>   Time speedup: 13.478"},{"location":"vqe/#optimization-with-parameter-shift-rule","title":"Optimization with parameter-shift rule","text":"<p>When using parameter shift rule (PSR), we can either use the same <code>expectation</code> using <code>diff_mode=horqrux.DiffMode.GPSR</code> or the functions:</p> <ul> <li><code>horqrux.differentiation.gpsr.jitted_analytical_exp</code> and <code>horqrux.differentiation.gpsr.jitted_finite_shots</code> as forward methods</li> <li><code>horqrux.differentiation.gpsr.analytical_gpsr_bwd</code> and <code>horqrux.differentiation.gpsr.finite_shots_gpsr_backward</code> as backward methods.</li> </ul> <p>Depending on the case, either way may be faster but the with the <code>expectation</code> we can obtain higher-order derivatives.</p>"},{"location":"vqe/#analytical","title":"Analytical","text":"<p>Let us rewrite our example using <code>jitted_analytical_exp</code> and <code>analytical_gpsr_bwd</code> for the analytical version of PSR:</p> <pre><code>from horqrux.differentiation.gpsr import jitted_analytical_exp, analytical_gpsr_bwd\n\n# Create random initial values for the parameters\nkey = jax.random.PRNGKey(42)\ninit_param_vals = jax.random.uniform(key, shape=(ansatz.n_vparams,))\n\noptimizer = optax.adam(learning_rate=LEARNING_RATE)\nansatz_ops = list(iter(ansatz))\n\ndef optimize_step(param_vals: Array, opt_state: Array, grads: dict[str, Array]) -&gt; tuple:\n    updates, opt_state = optimizer.update(grads, opt_state, param_vals)\n    param_vals = optax.apply_updates(param_vals, updates)\n    return param_vals, opt_state\n\ndef loss_fn(param_vals: Array) -&gt; Array:\n    \"\"\"The loss function is the sum of all expectation value for the observable components.\"\"\"\n    values = dict(zip(ansatz.vparams, param_vals))\n    return jitted_analytical_exp(init_state, ansatz_ops, observables=[H2_hamiltonian], values=values).sum()\n\ndef bwd_loss_fn(param_vals: Array) -&gt; Array:\n    \"\"\"The backward returns directly the gradient vector via GPSR and `jitted_analytical_exp`.\"\"\"\n    values = dict(zip(ansatz.vparams, param_vals))\n    return analytical_gpsr_bwd(init_state, ansatz_ops, observables=[H2_hamiltonian], values=values)\n\ndef train_step(i: int, param_vals_opt_state: tuple) -&gt; tuple:\n    param_vals, opt_state = param_vals_opt_state\n    grads = bwd_loss_fn(param_vals)\n    return optimize_step(param_vals, opt_state, grads)\n\n# set initial parameters and the state of the optimizer\nparam_vals = init_param_vals.clone()\nopt_state = optimizer.init(init_param_vals)\n\ndef train_unjitted(param_vals, opt_state):\n    for i in range(0, N_EPOCHS):\n        param_vals, opt_state = train_step(i, (param_vals, opt_state))\n    return param_vals, opt_state\n\nparam_vals, opt_state = train_unjitted(param_vals, opt_state)\n</code></pre>   Final loss: -1.051"},{"location":"vqe/#with-shots","title":"With shots","text":"<p>Let us rewrite our example using <code>jitted_finite_shots</code> and <code>finite_shots_gpsr_backward</code> for the shot-based version of PSR:</p> <pre><code>from horqrux.differentiation.gpsr import jitted_finite_shots, finite_shots_gpsr_backward\n\n\n# Create random initial values for the parameters\nkey = jax.random.PRNGKey(42)\ninit_param_vals = jax.random.uniform(key, shape=(ansatz.n_vparams,))\n\noptimizer = optax.adam(learning_rate=LEARNING_RATE)\n\ndef optimize_step(param_vals: Array, opt_state: Array, grads: dict[str, Array]) -&gt; tuple:\n    updates, opt_state = optimizer.update(grads, opt_state, param_vals)\n    param_vals = optax.apply_updates(param_vals, updates)\n    return param_vals, opt_state\n\ndef loss_fn(param_vals: Array, key: jax.random.PRNGKey) -&gt; Array:\n    \"\"\"The loss function is the sum of all expectation value for the observable components.\"\"\"\n    values = dict(zip(ansatz.vparams, param_vals))\n    return jitted_finite_shots(init_state, ansatz_ops, observables=[H2_hamiltonian], values=values, n_shots=10000, key=key).sum()\n\ndef bwd_loss_fn(param_vals: Array, key: jax.random.PRNGKey) -&gt; Array:\n    values = dict(zip(ansatz.vparams, param_vals))\n    return finite_shots_gpsr_backward(init_state, ansatz_ops, observables=[H2_hamiltonian], values=values, n_shots=10000, key=key)\n\ndef train_step(i: int, param_vals_opt_state: tuple) -&gt; tuple:\n    param_vals, opt_state = param_vals_opt_state\n    grads = bwd_loss_fn(param_vals, jax.random.PRNGKey(i))\n    return optimize_step(param_vals, opt_state, grads)\n\n# set initial parameters and the state of the optimizer\nparam_vals = init_param_vals.clone()\nopt_state = optimizer.init(init_param_vals)\n\ndef train_unjitted(param_vals, opt_state):\n    for i in range(0, N_EPOCHS):\n        param_vals, opt_state = train_step(i, (param_vals, opt_state))\n    return param_vals, opt_state\n\nparam_vals, opt_state = train_unjitted(param_vals, opt_state)\n\ndef analytical_expectation(param_vals: Array) -&gt; Array:\n    values = dict(zip(ansatz.vparams, param_vals))\n    return jitted_analytical_exp(init_state, ansatz_ops, observables=[H2_hamiltonian], values=values).sum()\n</code></pre>   Final loss: -1.050    <ol> <li> <p>Tilly et al., The Variational Quantum Eigensolver: a review of methods and best practices (2022) \u21a9</p> </li> <li> <p>Pennylane, Quantum Datasets \u21a9</p> </li> <li> <p>Kandala et al., Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets (2017) \u21a9</p> </li> </ol>"}]}